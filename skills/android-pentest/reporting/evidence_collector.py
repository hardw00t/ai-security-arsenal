#!/usr/bin/env python3
"""
Evidence Collector
Automatically organizes and formats evidence for Android penetration test reports.
"""

import base64
import hashlib
import json
import os
import re
import shutil
from dataclasses import dataclass, field, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Optional
import mimetypes


class EvidenceType(Enum):
    """Types of evidence."""
    SCREENSHOT = "screenshot"
    LOGCAT = "logcat"
    CODE = "code"
    REQUEST = "request"
    RESPONSE = "response"
    FRIDA_OUTPUT = "frida_output"
    FILE = "file"
    DATABASE = "database"
    NETWORK_CAPTURE = "network_capture"
    CONFIGURATION = "configuration"
    COMMAND_OUTPUT = "command_output"


@dataclass
class Evidence:
    """Represents a piece of evidence."""
    id: str
    type: EvidenceType
    title: str
    description: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    source_path: Optional[str] = None
    content: Optional[str] = None
    content_hash: Optional[str] = None
    mime_type: Optional[str] = None
    metadata: dict = field(default_factory=dict)
    finding_ids: list[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        """Convert to dictionary."""
        data = asdict(self)
        data['type'] = self.type.value
        return data


@dataclass
class TimelineEvent:
    """Represents an event in the testing timeline."""
    timestamp: str
    event_type: str
    description: str
    evidence_ids: list[str] = field(default_factory=list)
    finding_ids: list[str] = field(default_factory=list)

    def to_dict(self) -> dict:
        return asdict(self)


class EvidenceCollector:
    """Collects and organizes evidence for penetration test reports."""

    def __init__(self, output_dir: Path, package_name: str):
        self.output_dir = Path(output_dir)
        self.package_name = package_name
        self.evidence_dir = self.output_dir / "evidence"
        self.evidence_items: list[Evidence] = []
        self.timeline: list[TimelineEvent] = []

        # Create directory structure
        self._setup_directories()

        # Load existing evidence if any
        self._load_existing()

    def _setup_directories(self) -> None:
        """Create evidence directory structure."""
        subdirs = [
            "screenshots",
            "logcat",
            "frida",
            "network",
            "files",
            "databases",
            "code",
            "requests"
        ]

        for subdir in subdirs:
            (self.evidence_dir / subdir).mkdir(parents=True, exist_ok=True)

    def _load_existing(self) -> None:
        """Load existing evidence catalog."""
        catalog_path = self.evidence_dir / "catalog.json"
        if catalog_path.exists():
            try:
                with open(catalog_path) as f:
                    data = json.load(f)
                    for item in data.get('evidence', []):
                        item['type'] = EvidenceType(item['type'])
                        self.evidence_items.append(Evidence(**item))
                    for event in data.get('timeline', []):
                        self.timeline.append(TimelineEvent(**event))
            except Exception as e:
                print(f"Warning: Could not load existing evidence catalog: {e}")

    def _save_catalog(self) -> None:
        """Save evidence catalog."""
        catalog = {
            "package": self.package_name,
            "updated": datetime.now().isoformat(),
            "evidence_count": len(self.evidence_items),
            "evidence": [e.to_dict() for e in self.evidence_items],
            "timeline": [t.to_dict() for t in self.timeline]
        }

        with open(self.evidence_dir / "catalog.json", 'w') as f:
            json.dump(catalog, f, indent=2)

    def _generate_id(self, prefix: str) -> str:
        """Generate unique evidence ID."""
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        counter = len([e for e in self.evidence_items if e.id.startswith(prefix)])
        return f"{prefix}_{timestamp}_{counter:03d}"

    def _hash_content(self, content: bytes) -> str:
        """Generate SHA-256 hash of content."""
        return hashlib.sha256(content).hexdigest()

    def add_screenshot(self, path: Path, title: str, description: str,
                      finding_ids: list[str] = None) -> Evidence:
        """Add a screenshot as evidence."""
        evidence_id = self._generate_id("SCR")
        dest_path = self.evidence_dir / "screenshots" / f"{evidence_id}{path.suffix}"

        # Copy file
        shutil.copy2(path, dest_path)

        # Calculate hash
        with open(dest_path, 'rb') as f:
            content_hash = self._hash_content(f.read())

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.SCREENSHOT,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content_hash=content_hash,
            mime_type=mimetypes.guess_type(str(path))[0] or "image/png",
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("screenshot_captured", f"Screenshot: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def add_logcat(self, content: str, title: str, description: str,
                  finding_ids: list[str] = None, highlights: list[str] = None) -> Evidence:
        """Add logcat output as evidence."""
        evidence_id = self._generate_id("LOG")

        # Format and highlight logcat
        formatted = self._format_logcat(content, highlights or [])

        # Save to file
        dest_path = self.evidence_dir / "logcat" / f"{evidence_id}.txt"
        with open(dest_path, 'w') as f:
            f.write(formatted)

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.LOGCAT,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=formatted[:5000] if len(formatted) > 5000 else formatted,
            content_hash=self._hash_content(formatted.encode()),
            mime_type="text/plain",
            metadata={"highlights": highlights or [], "line_count": len(content.splitlines())},
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("logcat_captured", f"Logcat: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def _format_logcat(self, content: str, highlights: list[str]) -> str:
        """Format logcat output with highlights."""
        lines = content.splitlines()
        formatted_lines = []

        for line in lines:
            # Add marker for highlighted lines
            is_highlighted = any(h.lower() in line.lower() for h in highlights)
            prefix = ">>> " if is_highlighted else "    "
            formatted_lines.append(f"{prefix}{line}")

        return '\n'.join(formatted_lines)

    def add_frida_output(self, content: str, script_name: str, title: str,
                        description: str, finding_ids: list[str] = None) -> Evidence:
        """Add Frida script output as evidence."""
        evidence_id = self._generate_id("FRD")

        # Format output
        formatted = self._format_frida_output(content, script_name)

        # Save to file
        dest_path = self.evidence_dir / "frida" / f"{evidence_id}.json"
        with open(dest_path, 'w') as f:
            f.write(formatted)

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.FRIDA_OUTPUT,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=formatted[:5000] if len(formatted) > 5000 else formatted,
            content_hash=self._hash_content(formatted.encode()),
            mime_type="application/json",
            metadata={"script_name": script_name},
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("frida_captured", f"Frida: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def _format_frida_output(self, content: str, script_name: str) -> str:
        """Format Frida output for readability."""
        # Try to parse as JSON array of messages
        try:
            messages = []
            for line in content.splitlines():
                if line.strip():
                    try:
                        msg = json.loads(line)
                        messages.append(msg)
                    except json.JSONDecodeError:
                        messages.append({"raw": line})

            output = {
                "script": script_name,
                "timestamp": datetime.now().isoformat(),
                "message_count": len(messages),
                "messages": messages
            }
            return json.dumps(output, indent=2)
        except Exception:
            return json.dumps({
                "script": script_name,
                "timestamp": datetime.now().isoformat(),
                "raw_output": content
            }, indent=2)

    def add_request_response(self, request: str, response: str, url: str,
                            title: str, description: str,
                            finding_ids: list[str] = None) -> Evidence:
        """Add HTTP request/response pair as evidence."""
        evidence_id = self._generate_id("HTTP")

        # Format as readable document
        formatted = self._format_http_pair(request, response, url)

        # Save to file
        dest_path = self.evidence_dir / "requests" / f"{evidence_id}.txt"
        with open(dest_path, 'w') as f:
            f.write(formatted)

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.REQUEST,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=formatted[:10000] if len(formatted) > 10000 else formatted,
            content_hash=self._hash_content(formatted.encode()),
            mime_type="text/plain",
            metadata={"url": url},
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("http_captured", f"HTTP: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def _format_http_pair(self, request: str, response: str, url: str) -> str:
        """Format HTTP request/response pair."""
        separator = "=" * 80

        output = f"""
{separator}
URL: {url}
Captured: {datetime.now().isoformat()}
{separator}

=== REQUEST ===
{request}

=== RESPONSE ===
{response}

{separator}
"""
        return output.strip()

    def add_code_snippet(self, code: str, language: str, filename: str,
                        title: str, description: str,
                        line_numbers: tuple[int, int] = None,
                        finding_ids: list[str] = None) -> Evidence:
        """Add code snippet as evidence."""
        evidence_id = self._generate_id("CODE")

        # Format with line numbers
        formatted = self._format_code(code, language, filename, line_numbers)

        # Save to file
        ext = {"java": ".java", "kotlin": ".kt", "xml": ".xml", "json": ".json"}.get(language, ".txt")
        dest_path = self.evidence_dir / "code" / f"{evidence_id}{ext}"
        with open(dest_path, 'w') as f:
            f.write(formatted)

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.CODE,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=formatted,
            content_hash=self._hash_content(code.encode()),
            mime_type=f"text/{language}" if language else "text/plain",
            metadata={
                "language": language,
                "filename": filename,
                "line_range": line_numbers
            },
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("code_captured", f"Code: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def _format_code(self, code: str, language: str, filename: str,
                    line_numbers: tuple[int, int] = None) -> str:
        """Format code with metadata header and line numbers."""
        lines = code.splitlines()
        start_line = line_numbers[0] if line_numbers else 1

        header = f"""// File: {filename}
// Language: {language}
// Lines: {start_line} - {start_line + len(lines) - 1}
// Captured: {datetime.now().isoformat()}
// {'=' * 60}

"""

        numbered_lines = []
        for i, line in enumerate(lines, start=start_line):
            numbered_lines.append(f"{i:4d} | {line}")

        return header + '\n'.join(numbered_lines)

    def add_file(self, path: Path, title: str, description: str,
                finding_ids: list[str] = None) -> Evidence:
        """Add a file as evidence."""
        evidence_id = self._generate_id("FILE")
        dest_path = self.evidence_dir / "files" / f"{evidence_id}_{path.name}"

        # Copy file
        shutil.copy2(path, dest_path)

        # Read content if text file
        content = None
        mime_type = mimetypes.guess_type(str(path))[0]
        if mime_type and mime_type.startswith('text'):
            try:
                with open(dest_path, 'r', errors='ignore') as f:
                    content = f.read(10000)  # Limit to 10KB
            except Exception:
                pass

        # Calculate hash
        with open(dest_path, 'rb') as f:
            content_hash = self._hash_content(f.read())

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.FILE,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=content,
            content_hash=content_hash,
            mime_type=mime_type,
            metadata={"original_name": path.name, "size": path.stat().st_size},
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("file_collected", f"File: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def add_database_extract(self, db_path: Path, tables: list[str],
                            title: str, description: str,
                            finding_ids: list[str] = None) -> Evidence:
        """Add database extraction as evidence."""
        evidence_id = self._generate_id("DB")
        dest_path = self.evidence_dir / "databases" / f"{evidence_id}_{db_path.name}"

        # Copy database
        shutil.copy2(db_path, dest_path)

        # Calculate hash
        with open(dest_path, 'rb') as f:
            content_hash = self._hash_content(f.read())

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.DATABASE,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content_hash=content_hash,
            mime_type="application/x-sqlite3",
            metadata={
                "original_name": db_path.name,
                "tables": tables,
                "size": db_path.stat().st_size
            },
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("database_extracted", f"Database: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def add_network_capture(self, pcap_path: Path, title: str, description: str,
                           packet_count: int = 0, finding_ids: list[str] = None) -> Evidence:
        """Add network capture (pcap) as evidence."""
        evidence_id = self._generate_id("PCAP")
        dest_path = self.evidence_dir / "network" / f"{evidence_id}.pcap"

        # Copy file
        shutil.copy2(pcap_path, dest_path)

        # Calculate hash
        with open(dest_path, 'rb') as f:
            content_hash = self._hash_content(f.read())

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.NETWORK_CAPTURE,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content_hash=content_hash,
            mime_type="application/vnd.tcpdump.pcap",
            metadata={
                "packet_count": packet_count,
                "size": pcap_path.stat().st_size
            },
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("network_captured", f"PCAP: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def add_command_output(self, command: str, output: str, title: str,
                          description: str, finding_ids: list[str] = None) -> Evidence:
        """Add command output as evidence."""
        evidence_id = self._generate_id("CMD")

        # Format output
        formatted = f"""Command: {command}
Executed: {datetime.now().isoformat()}
{'=' * 60}

{output}
"""

        # Save to file
        dest_path = self.evidence_dir / "files" / f"{evidence_id}_output.txt"
        with open(dest_path, 'w') as f:
            f.write(formatted)

        evidence = Evidence(
            id=evidence_id,
            type=EvidenceType.COMMAND_OUTPUT,
            title=title,
            description=description,
            source_path=str(dest_path.relative_to(self.output_dir)),
            content=formatted[:5000] if len(formatted) > 5000 else formatted,
            content_hash=self._hash_content(formatted.encode()),
            mime_type="text/plain",
            metadata={"command": command},
            finding_ids=finding_ids or []
        )

        self.evidence_items.append(evidence)
        self._add_timeline_event("command_executed", f"Command: {title}", [evidence_id], finding_ids)
        self._save_catalog()

        return evidence

    def _add_timeline_event(self, event_type: str, description: str,
                           evidence_ids: list[str] = None,
                           finding_ids: list[str] = None) -> None:
        """Add event to timeline."""
        event = TimelineEvent(
            timestamp=datetime.now().isoformat(),
            event_type=event_type,
            description=description,
            evidence_ids=evidence_ids or [],
            finding_ids=finding_ids or []
        )
        self.timeline.append(event)

    def get_evidence_for_finding(self, finding_id: str) -> list[Evidence]:
        """Get all evidence associated with a finding."""
        return [e for e in self.evidence_items if finding_id in e.finding_ids]

    def get_timeline(self, start_date: str = None, end_date: str = None) -> list[TimelineEvent]:
        """Get timeline events, optionally filtered by date range."""
        events = self.timeline

        if start_date:
            events = [e for e in events if e.timestamp >= start_date]
        if end_date:
            events = [e for e in events if e.timestamp <= end_date]

        return sorted(events, key=lambda x: x.timestamp)

    def generate_evidence_summary(self) -> dict:
        """Generate summary of collected evidence."""
        by_type = {}
        for e in self.evidence_items:
            type_name = e.type.value
            if type_name not in by_type:
                by_type[type_name] = []
            by_type[type_name].append(e.id)

        return {
            "total_items": len(self.evidence_items),
            "by_type": {k: len(v) for k, v in by_type.items()},
            "timeline_events": len(self.timeline),
            "earliest_event": self.timeline[0].timestamp if self.timeline else None,
            "latest_event": self.timeline[-1].timestamp if self.timeline else None
        }

    def export_for_report(self) -> dict:
        """Export evidence data for report generation."""
        return {
            "evidence": [e.to_dict() for e in self.evidence_items],
            "timeline": [t.to_dict() for t in self.timeline],
            "summary": self.generate_evidence_summary()
        }


def collect_screenshots_from_dir(collector: EvidenceCollector, screenshot_dir: Path,
                                 finding_id: str = None) -> list[Evidence]:
    """Batch collect screenshots from a directory."""
    evidence_list = []
    supported_formats = {'.png', '.jpg', '.jpeg', '.gif', '.webp'}

    for path in sorted(screenshot_dir.iterdir()):
        if path.suffix.lower() in supported_formats:
            evidence = collector.add_screenshot(
                path=path,
                title=path.stem.replace('_', ' ').title(),
                description=f"Screenshot captured during testing: {path.name}",
                finding_ids=[finding_id] if finding_id else []
            )
            evidence_list.append(evidence)

    return evidence_list


def parse_logcat_file(collector: EvidenceCollector, logcat_path: Path,
                     sensitive_patterns: list[str] = None) -> Evidence:
    """Parse and add logcat file as evidence."""
    default_patterns = ['password', 'token', 'key', 'secret', 'auth', 'credential']
    highlights = sensitive_patterns or default_patterns

    with open(logcat_path) as f:
        content = f.read()

    return collector.add_logcat(
        content=content,
        title="Application Logcat Capture",
        description="Logcat output captured during application testing",
        highlights=highlights
    )


if __name__ == "__main__":
    # Example usage
    collector = EvidenceCollector(
        output_dir=Path("./test_output"),
        package_name="com.example.app"
    )

    # Add some test evidence
    collector.add_logcat(
        content="12-01 10:00:00.000 D/App: User logged in\n12-01 10:00:01.000 D/App: Token: abc123secret",
        title="Login Flow Logcat",
        description="Captured logcat during login testing showing token exposure",
        highlights=["token", "secret"],
        finding_ids=["VULN001"]
    )

    collector.add_code_snippet(
        code='val password = "hardcoded123"\nLog.d(TAG, "Password: $password")',
        language="kotlin",
        filename="LoginActivity.kt",
        title="Hardcoded Password",
        description="Hardcoded password found in login activity",
        line_numbers=(45, 46),
        finding_ids=["VULN002"]
    )

    # Print summary
    print(json.dumps(collector.generate_evidence_summary(), indent=2))
