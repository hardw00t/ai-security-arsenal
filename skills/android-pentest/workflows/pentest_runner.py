#!/usr/bin/env python3
"""
Pentest Runner - Master Workflow Orchestrator
Execute multiple pentesting workflows with consolidated reporting.

Usage:
    python pentest_runner.py --package com.target.app --workflows all
    python pentest_runner.py -p com.target.app --workflows auth,crypto,network --output ./results/
    python pentest_runner.py -p com.target.app --workflows full --parallel
"""

import argparse
import json
import os
import sys
import time
import importlib
import concurrent.futures
from datetime import datetime
from pathlib import Path
from typing import Optional

# Add workflows directory to path
sys.path.insert(0, str(Path(__file__).parent))

from base import Finding, Severity, FindingCategory


class PentestRunner:
    """Master workflow orchestrator for Android pentesting."""

    # Available workflows
    WORKFLOWS = {
        'full': {
            'module': 'full_assessment',
            'class': 'FullAssessmentWorkflow',
            'description': 'Complete OWASP MASTG assessment',
            'priority': 1
        },
        'auth': {
            'module': 'auth_bypass_scanner',
            'class': 'AuthBypassScanner',
            'description': 'Authentication bypass testing',
            'priority': 2
        },
        'data': {
            'module': 'data_leakage_scanner',
            'class': 'DataLeakageScanner',
            'description': 'Data leakage detection',
            'priority': 3
        },
        'crypto': {
            'module': 'crypto_auditor',
            'class': 'CryptoAuditor',
            'description': 'Cryptography auditing',
            'priority': 4
        },
        'network': {
            'module': 'network_security_scanner',
            'class': 'NetworkSecurityScanner',
            'description': 'Network security testing',
            'priority': 5
        },
        'components': {
            'module': 'component_fuzzer',
            'class': 'ComponentFuzzer',
            'description': 'Component fuzzing',
            'priority': 6
        },
        'privacy': {
            'module': 'privacy_analyzer',
            'class': 'PrivacyAnalyzer',
            'description': 'Privacy analysis',
            'priority': 7
        }
    }

    # Workflow groups
    WORKFLOW_GROUPS = {
        'all': list(WORKFLOWS.keys()),
        'quick': ['auth', 'data', 'network'],
        'security': ['auth', 'crypto', 'network', 'components'],
        'compliance': ['full', 'privacy', 'data'],
    }

    def __init__(self, package: str, output_dir: Path, apk_path: Optional[Path] = None,
                 workflows: list[str] = None, parallel: bool = False,
                 resume: bool = False, verbose: bool = True,
                 proxy: str = None, device: str = None):
        self.package = package
        self.output_dir = Path(output_dir)
        self.apk_path = apk_path
        self.workflows = self._resolve_workflows(workflows or ['all'])
        self.parallel = parallel
        self.resume = resume
        self.verbose = verbose
        self.proxy = proxy
        self.device = device

        # Results tracking
        self.results: dict[str, dict] = {}
        self.all_findings: list[dict] = []
        self.start_time: Optional[float] = None
        self.end_time: Optional[float] = None

        # Create output directory
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # State file for resume capability
        self.state_file = self.output_dir / 'runner_state.json'

    def _resolve_workflows(self, workflows: list[str]) -> list[str]:
        """Resolve workflow names, expanding groups."""
        resolved = []

        for wf in workflows:
            wf = wf.lower()

            if wf in self.WORKFLOW_GROUPS:
                resolved.extend(self.WORKFLOW_GROUPS[wf])
            elif wf in self.WORKFLOWS:
                resolved.append(wf)
            else:
                print(f"Warning: Unknown workflow '{wf}', skipping")

        # Remove duplicates while preserving order
        seen = set()
        unique = []
        for wf in resolved:
            if wf not in seen:
                seen.add(wf)
                unique.append(wf)

        # Sort by priority
        unique.sort(key=lambda w: self.WORKFLOWS[w]['priority'])

        return unique

    def _load_state(self) -> dict:
        """Load previous state if resuming."""
        if self.resume and self.state_file.exists():
            try:
                with open(self.state_file) as f:
                    return json.load(f)
            except Exception as e:
                print(f"Warning: Could not load state: {e}")
        return {'completed': [], 'results': {}}

    def _save_state(self, completed: list[str]) -> None:
        """Save current state for resume capability."""
        state = {
            'package': self.package,
            'workflows': self.workflows,
            'completed': completed,
            'results': self.results,
            'timestamp': datetime.now().isoformat()
        }
        with open(self.state_file, 'w') as f:
            json.dump(state, f, indent=2)

    def _run_workflow(self, workflow_name: str) -> dict:
        """Run a single workflow and return results."""
        workflow_info = self.WORKFLOWS[workflow_name]

        print(f"\n{'='*60}")
        print(f"Running: {workflow_info['description']}")
        print(f"{'='*60}\n")

        try:
            # Dynamically import the workflow module
            module = importlib.import_module(workflow_info['module'])
            workflow_class = getattr(module, workflow_info['class'])

            # Create workflow instance with appropriate parameters
            workflow_kwargs = {
                'package': self.package,
                'output_dir': self.output_dir / workflow_name,
                'apk_path': self.apk_path,
                'resume': self.resume,
                'verbose': self.verbose
            }

            # Add workflow-specific parameters
            if workflow_name == 'network' and self.proxy:
                workflow_kwargs['proxy'] = self.proxy

            workflow = workflow_class(**workflow_kwargs)

            # Run the workflow
            findings = workflow.run()

            return {
                'success': True,
                'findings': [f.to_dict() for f in findings],
                'finding_count': len(findings),
                'critical': len([f for f in findings if f.severity == Severity.CRITICAL]),
                'high': len([f for f in findings if f.severity == Severity.HIGH]),
                'medium': len([f for f in findings if f.severity == Severity.MEDIUM]),
                'low': len([f for f in findings if f.severity == Severity.LOW]),
                'info': len([f for f in findings if f.severity == Severity.INFO])
            }

        except Exception as e:
            print(f"Error running {workflow_name}: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'findings': [],
                'finding_count': 0
            }

    def run(self) -> dict:
        """Execute all selected workflows."""
        self.start_time = time.time()

        # Load previous state if resuming
        state = self._load_state()
        completed = state.get('completed', [])

        # Determine which workflows to run
        workflows_to_run = [w for w in self.workflows if w not in completed]

        if not workflows_to_run:
            print("All workflows already completed. Use --no-resume to start fresh.")
            return state.get('results', {})

        print(f"\n{'#'*60}")
        print(f"# Android Penetration Testing Runner")
        print(f"# Target: {self.package}")
        print(f"# Workflows: {', '.join(workflows_to_run)}")
        print(f"# Output: {self.output_dir}")
        print(f"{'#'*60}\n")

        if self.parallel:
            # Run workflows in parallel
            print("Running workflows in parallel...")
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                future_to_workflow = {
                    executor.submit(self._run_workflow, wf): wf
                    for wf in workflows_to_run
                }

                for future in concurrent.futures.as_completed(future_to_workflow):
                    workflow_name = future_to_workflow[future]
                    try:
                        result = future.result()
                        self.results[workflow_name] = result
                        completed.append(workflow_name)
                        self._save_state(completed)

                        if result['findings']:
                            self.all_findings.extend(result['findings'])

                    except Exception as e:
                        print(f"Workflow {workflow_name} failed: {e}")
                        self.results[workflow_name] = {'success': False, 'error': str(e)}
        else:
            # Run workflows sequentially
            for workflow_name in workflows_to_run:
                result = self._run_workflow(workflow_name)
                self.results[workflow_name] = result
                completed.append(workflow_name)
                self._save_state(completed)

                if result['findings']:
                    self.all_findings.extend(result['findings'])

        self.end_time = time.time()

        # Generate consolidated report
        self._generate_consolidated_report()

        return self.results

    def _generate_consolidated_report(self) -> None:
        """Generate a consolidated report from all workflows."""
        print(f"\n{'='*60}")
        print("Generating Consolidated Report")
        print(f"{'='*60}\n")

        # Deduplicate findings by ID
        unique_findings = {}
        for finding in self.all_findings:
            finding_id = finding.get('id', '')
            if finding_id not in unique_findings:
                unique_findings[finding_id] = finding

        findings_list = list(unique_findings.values())

        # Calculate summary statistics
        total_findings = len(findings_list)
        critical = len([f for f in findings_list if f.get('severity') == 'critical'])
        high = len([f for f in findings_list if f.get('severity') == 'high'])
        medium = len([f for f in findings_list if f.get('severity') == 'medium'])
        low = len([f for f in findings_list if f.get('severity') == 'low'])
        info = len([f for f in findings_list if f.get('severity') == 'info'])

        # Calculate risk score
        if critical > 0:
            risk_level = 'CRITICAL'
        elif high > 2:
            risk_level = 'HIGH'
        elif high > 0 or medium > 3:
            risk_level = 'MEDIUM'
        elif medium > 0:
            risk_level = 'LOW'
        else:
            risk_level = 'MINIMAL'

        # Group findings by category
        by_category = {}
        for finding in findings_list:
            category = finding.get('category', 'UNKNOWN')
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(finding)

        # Build consolidated report
        report = {
            'assessment': {
                'type': 'Android Penetration Test',
                'package': self.package,
                'apk_path': str(self.apk_path) if self.apk_path else None,
                'started': datetime.fromtimestamp(self.start_time).isoformat() if self.start_time else None,
                'completed': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
                'duration_seconds': round(self.end_time - self.start_time, 2) if self.end_time and self.start_time else None
            },
            'summary': {
                'risk_level': risk_level,
                'total_findings': total_findings,
                'by_severity': {
                    'critical': critical,
                    'high': high,
                    'medium': medium,
                    'low': low,
                    'info': info
                },
                'by_category': {cat: len(findings) for cat, findings in by_category.items()}
            },
            'workflows_executed': {
                name: {
                    'success': result.get('success', False),
                    'finding_count': result.get('finding_count', 0),
                    'error': result.get('error')
                }
                for name, result in self.results.items()
            },
            'findings_by_severity': {
                'critical': [f for f in findings_list if f.get('severity') == 'critical'],
                'high': [f for f in findings_list if f.get('severity') == 'high'],
                'medium': [f for f in findings_list if f.get('severity') == 'medium'],
                'low': [f for f in findings_list if f.get('severity') == 'low'],
                'info': [f for f in findings_list if f.get('severity') == 'info']
            },
            'findings_by_category': by_category,
            'all_findings': findings_list,
            'executive_summary': self._generate_executive_summary(
                risk_level, total_findings, critical, high, medium, findings_list
            )
        }

        # Save JSON report
        report_path = self.output_dir / f"{self.package}_consolidated_report.json"
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(f"Consolidated report saved to: {report_path}")

        # Print summary
        self._print_summary(risk_level, total_findings, critical, high, medium, low, info)

    def _generate_executive_summary(self, risk_level: str, total: int,
                                   critical: int, high: int, medium: int,
                                   findings: list[dict]) -> str:
        """Generate an executive summary."""
        summary_parts = [
            f"Security assessment of {self.package} identified {total} findings.",
            f"Overall risk level: {risk_level}.",
        ]

        if critical > 0:
            summary_parts.append(f"CRITICAL: {critical} critical vulnerabilities require immediate attention.")

        if high > 0:
            summary_parts.append(f"HIGH: {high} high-risk issues should be addressed urgently.")

        if medium > 0:
            summary_parts.append(f"MEDIUM: {medium} medium-risk findings should be remediated.")

        # Top recommendations
        top_findings = sorted(
            [f for f in findings if f.get('severity') in ['critical', 'high']],
            key=lambda x: 0 if x.get('severity') == 'critical' else 1
        )[:5]

        if top_findings:
            summary_parts.append("\nPriority Remediations:")
            for i, finding in enumerate(top_findings, 1):
                summary_parts.append(f"{i}. [{finding.get('severity', '').upper()}] {finding.get('title', '')}")

        return '\n'.join(summary_parts)

    def _print_summary(self, risk_level: str, total: int, critical: int,
                      high: int, medium: int, low: int, info: int) -> None:
        """Print assessment summary to console."""
        duration = round(self.end_time - self.start_time, 2) if self.end_time and self.start_time else 0

        risk_colors = {
            'CRITICAL': 'ðŸ”´',
            'HIGH': 'ðŸŸ ',
            'MEDIUM': 'ðŸŸ¡',
            'LOW': 'ðŸ”µ',
            'MINIMAL': 'ðŸŸ¢'
        }

        print(f"\n{'#'*60}")
        print(f"# ASSESSMENT COMPLETE")
        print(f"{'#'*60}")
        print(f"\nðŸ“¦ Package: {self.package}")
        print(f"â±ï¸  Duration: {duration}s")
        print(f"\n{risk_colors.get(risk_level, 'âšª')} Overall Risk: {risk_level}")
        print(f"\nðŸ“Š Findings Summary:")
        print(f"   ðŸ”´ Critical: {critical}")
        print(f"   ðŸŸ  High:     {high}")
        print(f"   ðŸŸ¡ Medium:   {medium}")
        print(f"   ðŸ”µ Low:      {low}")
        print(f"   âšª Info:     {info}")
        print(f"   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   Total:      {total}")
        print(f"\nðŸ“ Results: {self.output_dir}")
        print(f"{'#'*60}\n")


def main():
    parser = argparse.ArgumentParser(
        description='Android Penetration Testing Runner',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Workflow Options:
  all         - Run all workflows
  quick       - Run quick assessment (auth, data, network)
  security    - Run security-focused workflows
  compliance  - Run compliance-focused workflows

Individual Workflows:
  full        - Complete OWASP MASTG assessment
  auth        - Authentication bypass testing
  data        - Data leakage detection
  crypto      - Cryptography auditing
  network     - Network security testing
  components  - Component fuzzing
  privacy     - Privacy analysis

Examples:
  python pentest_runner.py -p com.app --workflows all
  python pentest_runner.py -p com.app --workflows auth,crypto,network
  python pentest_runner.py -p com.app --workflows quick --parallel
        """
    )

    parser.add_argument('--package', '-p', required=True, help='Target package name')
    parser.add_argument('--apk', '-a', help='Path to APK file')
    parser.add_argument('--output', '-o', default='./results', help='Output directory')
    parser.add_argument('--workflows', '-w', default='all',
                       help='Comma-separated workflows to run (default: all)')
    parser.add_argument('--parallel', action='store_true', help='Run workflows in parallel')
    parser.add_argument('--resume', '-r', action='store_true', help='Resume from previous state')
    parser.add_argument('--no-resume', action='store_true', help='Start fresh, ignore previous state')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    parser.add_argument('--proxy', help='Proxy address for network testing (host:port)')
    parser.add_argument('--device', '-d', help='Target device ID')
    parser.add_argument('--list-workflows', action='store_true', help='List available workflows')

    args = parser.parse_args()

    # List workflows and exit
    if args.list_workflows:
        print("\nAvailable Workflows:")
        print("-" * 50)
        for name, info in PentestRunner.WORKFLOWS.items():
            print(f"  {name:12} - {info['description']}")
        print("\nWorkflow Groups:")
        print("-" * 50)
        for group, workflows in PentestRunner.WORKFLOW_GROUPS.items():
            print(f"  {group:12} - {', '.join(workflows)}")
        return 0

    # Parse workflows
    workflows = args.workflows.split(',')

    # Create runner
    runner = PentestRunner(
        package=args.package,
        output_dir=Path(args.output),
        apk_path=Path(args.apk) if args.apk else None,
        workflows=workflows,
        parallel=args.parallel,
        resume=args.resume and not args.no_resume,
        verbose=args.verbose,
        proxy=args.proxy,
        device=args.device
    )

    # Run assessment
    results = runner.run()

    # Return exit code based on findings
    has_critical = any(r.get('critical', 0) > 0 for r in results.values())
    has_high = any(r.get('high', 0) > 0 for r in results.values())

    if has_critical:
        return 2  # Critical findings
    elif has_high:
        return 1  # High findings
    return 0  # Success


if __name__ == "__main__":
    exit(main())
